{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sktime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nHackathon - INCAP - IconPro GmbH\nTimeseries Classification with Transformers\n\"\"\"\nimport pandas as pd\nfrom tensorflow import keras\nfrom dataclasses import dataclass\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn.pipeline import make_pipeline\nfrom sktime.transformations.panel.rocket import Rocket, MiniRocket\n# Import packages as you need","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(data_path):\n    \"\"\"\n    Loading of the dataset provided\n    Edit the code below\n    \"\"\"\n    data = pd.read_pickle(data_path)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(data):\n    \"\"\"\n    A standard nan removal to be added.\n    Add more preprocessing steps if needed.\n    \"\"\"\n    \n    X = data['dim_0'].apply(lambda x: x.reshape(500,1))\n    \n    for i in range(data.shape[0]):\n        if True in np.isnan(data['dim_0'][i]).flatten():\n            print(i)\n            \n    input_x = []\n    for array in X:\n        input_x.append(array)\n    \n#     X = pd.DataFrame(data.dim_0.tolist())\n#     X = X.to_numpy()\n    \n    y = data['labels']\n    y = y.astype(int)\n    y[y == -1] = 0\n    return input_x,y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Rocket_preprocessing(input_x):\n    input_x1=[]\n    input_x = np.array(input_x).reshape(4921, 500)\n    print(input_x.shape)\n    for i in range(input_x.shape[0]):\n        input_x1.append(pd.Series(input_x[i]))\n    input_x1=pd.Series(input_x1)\n#    input_x1.shape\n    \n    input_x1=pd.DataFrame(input_x1)\n    return input_x1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_train_test(X, y):\n    \"\"\"\n    Splitting the data into train, test, validation \n    \"\"\"\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Rocket(X_train, X_val, X_test):\n    rocket = MiniRocket(num_kernels=500)  # by default, ROCKET uses 10,000 kernels\n    X_train = rocket.fit_transform(X_train)\n    X_train = np.expand_dims(np.array(X_train), axis=2)\n    print(X_train.shape)\n    \n    X_val = rocket.transform(X_val)\n    X_val = np.expand_dims(np.array(X_val), axis=2)\n    print(X_val.shape)\n    \n    X_test = rocket.transform(X_test)\n    X_test = np.expand_dims(np.array(X_test), axis=2)\n    print(X_test.shape)\n    \n    return X_train, X_val, X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalization(X_train, X_val, X_test):\n    scaler = StandardScaler()\n    shp = X_train.shape[1]\n    X_train = np.reshape(X_train, (-1,shp))\n    X_val = np.reshape(X_val, (-1,shp))\n    X_test = np.reshape(X_test, (-1,shp))\n    \n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    return np.reshape(X_train, (-1,shp,1)), np.reshape(X_val, (-1,shp,1)), np.reshape(X_test, (-1,shp,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timeseries_transform(data, head_size, num_heads, ff_dim, dropout=0):\n    \"\"\"\n    Implement the timeseries transformer here\n    \"\"\"\n    # Normalization and Attention\n    x = data\n    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    x = layers.Dropout(dropout)(x)\n    res = x + data\n\n    # Feed Forward Part\n    x = layers.LayerNormalization(epsilon=1e-6)(res)\n    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Conv1D(filters=data.shape[-1], kernel_size=1)(x)\n    return x + res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n    inputs = keras.Input(shape=input_shape)\n    x = inputs\n    for _ in range(num_transformer_blocks):\n        x = timeseries_transform(x, head_size, num_heads, ff_dim, dropout)\n\n    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"relu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n    outputs = layers.Dense(2, activation=\"sigmoid\")(x)\n    return keras.Model(inputs, outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_training(X_train, y_train, X_val, y_val):\n    \"\"\"\n    Train the data with the compatible model\n    \"\"\"\n    \n    input_shape = X_train.shape[1:]\n\n    model = build_model(input_shape, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[256], mlp_dropout=0.4, dropout=0.25)\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4, decay_steps=10000, decay_rate=0.9)\n\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n        metrics=[\"sparse_categorical_accuracy\"],\n    )\n    \n    model.summary()\n\n    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n\n    model.fit(X_train, y_train, validation_data=(X_val,y_val),  epochs=200, batch_size=128, callbacks=callbacks)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def metric(y_act, y_pred):\n    \"\"\"\n    Standard metrics and plotting should be same\n    Metrics should be computed on validation data(unseen data)\n    1. Balanced accuracy score\n    2. Confusion matrix\n    3. Per-class accuracy\n    \"\"\"\n    \n    cm = metrics.confusion_matrix(y_act, y_pred)\n    balanced_accuracy = metrics.balanced_accuracy_score(y_act, y_pred)\n    \n    return cm, balanced_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation(X_val, y_val, metrics):\n    \"\"\"\n    Comparing the results with provided Series Embedder\n    Plot confusion matrices of self analysis and LSTM with balanced_accuracy\n    \n    \"\"\"\n    \n    score = model.evaluate(X_val, y_val, verbose=1)\n    \n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(X_test, y_act, metric, model):\n    y_pred = model.predict(X_test, verbose=1)\n    y_pred = np.argmax(y_pred, axis=1)\n    cm, ba = metric(y_act, y_pred)\n    \n    return y_pred, cm, ba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/fordadata/data.pkl\"\ndata = load_data(path)\nX, y = preprocess_data(data)\nX = Rocket_preprocessing(X)\n\nX_train, X_val, X_test, y_train, y_val, y_test = split_train_test(X, y)\nX_train, X_val, X_test = Rocket(X_train, X_val, X_test)\nX_train, X_val, X_test = normalization(X_train, X_val, X_test)\nmodel_self=model_training(X_train, y_train, X_val, y_val)\n\nevaluate(X_test, y_test, metric, model_self)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_self.save(\"transformer_banana_muffin_normalized\")\n!zip -r transformer_normalized.zip \"/kaggle/working/transformer_banana_muffin_normalized\nmodel_self.save(\"transformer_banana_muffin_normalized.h5\")\n!zip -r transformer_normalized.zip \"/kaggle/working/transformer_banana_muffin_normalized\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics=metric(val,model_self)\n\n# lstm_cm,lstm_balanced_accuracy=lstm(preprocessed_data,target='labels')\n# metrics_validation = [lstm_cm, lstm_balanced_accuracy]\n# validation(metrics,metrics_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}